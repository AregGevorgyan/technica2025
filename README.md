<div align="center">

# ğŸ‘ï¸ Eye AAC: Adaptive AAC Eye Tracking System

# Website: [eye-acc.select](http://eye-acc.select/)

### Intelligent, Hands-Free Communication for Enhanced Accessibility

[![Next.js](https://img.shields.io/badge/Next.js-16.0-black?style=for-the-badge&logo=next.js)](https://nextjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue?style=for-the-badge&logo=typescript)](https://www.typescriptlang.org/)
[![Tailwind CSS](https://img.shields.io/badge/Tailwind-4.0-38bdf8?style=for-the-badge&logo=tailwind-css)](https://tailwindcss.com/)
[![WebGazer.js](https://img.shields.io/badge/WebGazer.js-Eye_Tracking-purple?style=for-the-badge)](https://webgazer.cs.brown.edu/)

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Eye Tracking](#-eye-tracking-setup) â€¢ [Documentation](#-documentation)

</div>

---

## ğŸŒŸ Overview

An **adaptive AAC (Augmentative and Alternative Communication)** system designed for individuals with limited motor control. Using advanced **webcam-based eye tracking**, users can communicate entirely hands-free by simply looking at communication tiles.

Built for **Technica 2025**, this system combines cutting-edge eye tracking technology with an intuitive interface to make communication accessible to everyone.

## âœ¨ Features

### ğŸ¯ **Eye Tracking Communication**
- **Hands-Free Operation**: Communicate using only your eyes
- **25-Point Calibration**: Automatic calibration with EyeGesturesLite (~1 minute)
- **Dwell-Based Selection**: Look at a tile for 1.5 seconds to select it
- **Visual Feedback**: Real-time gaze cursor and progress indicators
- **Smooth Tracking**: Advanced EWMA smoothing algorithms for stable, accurate tracking
- **Debug Tools**: Optional heatmap overlay and camera preview

### ğŸ¤– **AI-Powered Predictions (NEW!)**
- **Claude Haiku Integration**: Fast, context-aware word suggestions
- **Smart Image Matching**: 60 pre-loaded wordimages automatically matched
- **Auto-Refresh**: Updates predictions after every 3 selections
- **Context-Aware**: Considers time of day, recent words, and conversation flow
- **Manual Regenerate**: Click button for fresh suggestions anytime
- **Affordable**: ~$0.001 per prediction request

### ğŸ–¼ï¸ **Visual Communication**
- **60 Wordimages**: Pre-loaded PNG symbols for common AAC vocabulary
  - Emotions, actions, pronouns, social phrases, needs, animals
- **Automatic Matching**: AI prioritizes words with available images
- **High-Quality Symbols**: Clear, recognizable icons
- **Fallback Support**: Text-only tiles when images unavailable

### ğŸ—£ï¸ **Communication Modes**
- **Immediate Mode**: Speak each word as you select it
- **Compose Mode**: Build complete sentences before speaking
  - Edit and refine your message
  - Copy to clipboard for external use
  - Clear text or backspace functionality

### ğŸ¨ **Responsive Design**
- **Mobile-First**: Works on all screen sizes (320px â†’ 1920px+)
- **Adaptive Layout**: Interface scales to fit any device
- **High Contrast**: Accessibility-focused design
- **Touch & Click**: Alternative input methods always available

### ğŸ”Š **Text-to-Speech**
- **Browser-Based TTS**: Built-in Web Speech API
- **Natural Voices**: Clear, understandable speech output
- **Instant Feedback**: Immediate or composed speech options

## ğŸš€ Quick Start

### Prerequisites

```bash
Node.js 18+ and npm
Modern web browser (Chrome recommended)
Webcam for eye tracking
Anthropic API Key (for AI predictions) - See setup guide below
```

### Installation

```bash
# Clone the repository
git clone https://github.com/AregGevorgyan/technica2025.git
cd technica2025

# Install dependencies
npm install

# Setup API key (REQUIRED for AI predictions)
# See API_KEY_SETUP.md for detailed instructions
# Quick version:
# 1. Copy .env.example to .env.local
# 2. Add your Anthropic API key to ANTHROPIC_API_KEY
# 3. Get key at: https://console.anthropic.com/settings/keys

# Run development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) and start communicating!

### ğŸ”‘ API Key Setup (Required)

**For AI-powered word predictions**, you need an Anthropic API key:

ğŸ“– **Quick Guide**: See [API_KEY_SETUP.md](./API_KEY_SETUP.md) (2-minute setup)
ğŸ“š **Full Guide**: See [SETUP_GUIDE.md](./SETUP_GUIDE.md) (comprehensive documentation)

**TL;DR**: Get your API key at [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys) and add it to `.env.local`

## ğŸ‘ï¸ Eye Tracking Setup

### Step 1: Navigate to Calibration
1. Visit `http://localhost:3000`
2. Click **"Eye Tracking Setup"** button
3. Allow webcam access when prompted

### Step 2: Calibrate Your Gaze
1. **5 calibration points** will appear on screen
2. Look directly at each **red dot**
3. Click **3 times** per dot while looking at it
4. Complete all 5 points (top-left, top-right, center, bottom-left, bottom-right)

### Step 3: Start Communicating
- **Progress bar** shows calibration completion
- After calibration, you'll be redirected automatically
- **Gaze at tiles** for 1.5 seconds to select them
- Blue ring and fill animation show dwell progress

### Tips for Best Results
- ğŸ’¡ Sit about **2 feet** from the screen
- ğŸ’¡ Ensure **good lighting** on your face
- ğŸ’¡ Keep your **head still** during use
- ğŸ’¡ **Recalibrate** if accuracy degrades

## ğŸ“± Usage

### Communication Interface

**Immediate Mode:**
```
Look at "I want" â†’ Speaks "I want"
Look at "water" â†’ Speaks "water"
Look at "please" â†’ Speaks "please"
```

**Compose Mode:**
```
Look at "I want" â†’ Added to text bar
Look at "water" â†’ Added to text bar
Look at "please" â†’ Added to text bar
Text bar shows: "I want water please"
Look at "Speak" button â†’ Speaks full sentence
```

### Navigation
- **ğŸ‘ï¸ Icon**: Indicates eye tracking is active
- **Categories**: Browse communication options
- **Regenerate**: Get new tile suggestions
- **Mode Toggle**: Switch between Immediate and Compose

## ğŸ› ï¸ Tech Stack

<div align="center">

| Category | Technology |
|----------|-----------|
| **Frontend** | Next.js 16, React 19, TypeScript 5.0 |
| **Styling** | Tailwind CSS 4.0 |
| **Eye Tracking** | WebGazer.js |
| **Text-to-Speech** | Web Speech API |
| **Database** | Supabase (PostgreSQL + pgvector) |
| **AI/ML** | Anthropic Claude, OpenAI Embeddings |
| **Deployment** | Vercel |

</div>

## ğŸ“‚ Project Structure

```
technica2025/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ (main)/
â”‚   â”‚   â”œâ”€â”€ calibrate/          # ğŸ‘ï¸ Eye tracking calibration
â”‚   â”‚   â””â”€â”€ communicate/        # ğŸ’¬ Main communication interface
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ predict/           # ğŸ¤– Claude AI predictions
â”‚   â”‚   â”œâ”€â”€ images/            # ğŸ–¼ï¸ Image search & caching
â”‚   â”‚   â”œâ”€â”€ tts/               # ğŸ”Š Text-to-speech
â”‚   â”‚   â””â”€â”€ memory/            # ğŸ§  User memory learning
â”‚   â””â”€â”€ page.tsx               # ğŸ  Landing page
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ communication/         # ğŸ¯ Tile grid, text display, regenerate
â”‚   â”œâ”€â”€ input/                 # ğŸ‘ï¸ Eye tracking handler
â”‚   â””â”€â”€ wordimages/            # ğŸ–¼ï¸ 60 PNG symbols (moved to /public)
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â”œâ”€â”€ eyegestures-init.ts # EyeGesturesLite initialization
â”‚   â”‚   â””â”€â”€ gaze-utils.ts      # Smoothing & dwell detection
â”‚   â”œâ”€â”€ symbols/
â”‚   â”‚   â””â”€â”€ wordimage-mapper.ts # ğŸ”— Word-to-image mapping
â”‚   â””â”€â”€ supabase/              # Database utilities
â”œâ”€â”€ public/
â”‚   â””â”€â”€ wordimages/            # ğŸ–¼ï¸ 60 PNG symbol images
â”œâ”€â”€ types/                     # TypeScript definitions
â”œâ”€â”€ .env.local                 # ğŸ”‘ API keys (create this)
â”œâ”€â”€ API_KEY_SETUP.md          # ğŸ“– Quick API setup guide
â””â”€â”€ SETUP_GUIDE.md            # ğŸ“š Comprehensive setup documentation
```

## ğŸ¨ Features Deep Dive

### Advanced Eye Tracking
- **Exponential Weighted Moving Average**: Combines simple moving average with exponential smoothing
- **Buffer Size**: 10 data points for optimal smoothness
- **20px Margin**: Increased tolerance for easier targeting
- **Alpha Value 0.3**: Balances responsiveness with stability

### Calibration UX
- **Responsive Dots**: Scale from 12px (mobile) to 20px (desktop)
- **Progress Tracking**: Real-time percentage and point counter
- **Bottom Progress Bar**: Doesn't obstruct calibration points
- **Visual States**: Red (active), Green (completed), Gray (inactive)

### Accessibility
- **WCAG Compliant**: High contrast, clear text
- **Responsive**: Works on all devices
- **Multiple Inputs**: Eye tracking, mouse, touch, keyboard
- **Visual Feedback**: Clear indication of selection progress

## ğŸ”® Future Enhancements

- [ ] **AI-Powered Predictions**: Context-aware word suggestions
- [ ] **User Memory System**: Learn communication patterns
- [ ] **OpenSymbols Integration**: Visual symbols with text
- [ ] **Multi-Language Support**: International accessibility
- [ ] **Offline Mode**: PWA with local functionality
- [ ] **Settings UI**: Adjustable dwell time, sensitivity
- [ ] **Keyboard Navigation**: Full keyboard accessibility

## ğŸ“Š Current Implementation Status

### âœ… Completed (Phase 1)
- [x] Eye tracking with EyeGesturesLite integration
- [x] 25-point automatic calibration
- [x] Dwell-based tile selection
- [x] Immediate and Compose speech modes
- [x] Text display with copy functionality
- [x] Fully responsive design (mobile â†’ desktop)
- [x] Advanced EWMA gaze smoothing
- [x] Visual feedback (gaze cursor, progress rings, heatmap)
- [x] **Claude Haiku AI predictions** â­ NEW!
- [x] **60 wordimages with automatic matching** â­ NEW!
- [x] **Auto-refresh predictions every 3 selections** â­ NEW!
- [x] **Context-aware word suggestions** â­ NEW!

### ğŸš§ In Progress (Phase 2)
- [ ] Database integration (Supabase setup complete)
- [ ] Memory learning system (user pattern analysis)
- [ ] External image API integration (Google/Unsplash fallback)
- [ ] Settings UI (adjustable parameters)

## ğŸ¤ Contributing

Contributions are welcome! This project was built for accessibility and can always be improved.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **[WebGazer.js](https://webgazer.cs.brown.edu/)** - Webcam eye tracking
- **[OpenSymbols](https://www.opensymbols.org/)** - AAC symbol library
- **[Anthropic](https://www.anthropic.com/)** - Claude AI
- **[Supabase](https://supabase.com/)** - Backend infrastructure
- **[Vercel](https://vercel.com/)** - Deployment platform

## ğŸ“ Support

Having issues? Check our guides:
- ğŸ”‘ **[API Key Setup](API_KEY_SETUP.md)** - Quick 2-minute guide (START HERE!)
- ğŸ“š **[Full Setup Guide](SETUP_GUIDE.md)** - Comprehensive documentation
- ğŸ“– [Eye Tracking Setup Guide](EYE_TRACKING_GUIDE.md)
- ğŸ› [Debugging Guide](EYE_TRACKING_DEBUG.md)
- ğŸ’¡ [Quick Start Guide](QUICKSTART.md)
- ğŸ“‹ [Full Project Documentation](CLAUDE.md)

## ğŸŒ Demo

Try it live: [Coming Soon]

---

<div align="center">

**Made with â¤ï¸ for Technica 2025**

Empowering communication through technology

[â¬† Back to Top](#-adaptive-aac-eye-tracking-system)

</div>
